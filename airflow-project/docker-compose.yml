services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      TZ: Asia:Seoul
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - /etc/localtime:/etc/localtime:ro
    networks:
      - airflow-network

  selenium:
      container_name: remote_chromedriver
      image: seleniarm/standalone-chromium:latest
      environment:
        - SE_MAX_SESSIONS=8
        - SE_NODE_MAX_SESSIONS=8
      ports:
        - 4444:4444
      restart: always
      networks:
        - airflow-network

  namenode:
    image: hadoop-image:latest
    container_name: namenode
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64     
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_SITE_dfs_replication=1
      - HDFS_SITE_dfs_namenode_name_dir=/hadoop/dfs/name
    command: ["bash", "-c", "if [ ! -d /hadoop/dfs/name/current ]; then hdfs namenode -format; fi && hdfs namenode"]
    ports:
      - "9870:9870"
    networks:
      - airflow-network
    volumes:
      - namenode-data:/hadoop/dfs/namenode
    user: root


  datanode:
    image: hadoop-image:latest
    container_name: datanode
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64      
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
      - HDFS_SITE_dfs_datanode_data_dir=/hadoop/dfs/data
    command: ["hdfs", "datanode"]
    networks:
      - airflow-network
    volumes:
      - datanode-data:/hadoop/dfs/datanode

  
  webserver:
    image: airflow-image:latest
    user: "${AIRFLOW_UID}:0"  # 로컬 UID를 컨테이너에 전달
    restart: always
    depends_on:
      - postgres
      - selenium
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "m8C8oCOLlhUHBYtLRGemJz0TBcxqGWduwEZkvxdS"
      SELENIUM_URL: "http://selenium:4444/wd/hub"
      PATH: $PATH:/home/airflow/.local/bin
      HADOOP_CONF_DIR: /usr/local/hadoop/etc/hadoop
    ports:
      - "8081:8080"
    volumes:
        - /Users/data-project/data-pipeline-project/airflow-project/shared:/shared
        - ./airflow.cfg:/opt/airflow/airflow.cfg
        - /etc/localtime:/etc/localtime:ro
        - /Users/data-project/data-pipeline-project:/opt/airflow/data-pipeline-project

    networks:
      - airflow-network
    command: ["airflow", "webserver"]
    

  scheduler:
    image: airflow-image:latest
    user: "${AIRFLOW_UID}:0"
    restart: always
    depends_on:
      - postgres
      - webserver
    environment:
      AIRFLOW_HOME: /opt/airflow
      AIRFLOW_UID: ${AIRFLOW_UID}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "m8C8oCOLlhUHBYtLRGemJz0TBcxqGWduwEZkvxdS"
      PATH: $PATH:/home/airflow/.local/bin

    volumes:
        - /Users/data-project/data-pipeline-project/airflow-project/shared:/shared
        - ./airflow.cfg:/opt/airflow/airflow.cfg
        - /etc/localtime:/etc/localtime:ro
        - /Users/data-project/data-pipeline-project:/opt/airflow/data-pipeline-project
        - /Users/data-project/data-pipeline-project/airflow-project/shared/dags:/opt/airflow/dags

    networks:
      - airflow-network
    command: scheduler

volumes:
  postgres-db-volume:
  namenode-data:  # NameNode 볼륨 정의
  datanode-data:  # DataNode 볼륨 정의

networks:
  airflow-network:
    driver: bridge